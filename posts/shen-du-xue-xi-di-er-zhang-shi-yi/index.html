<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="zh_cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>《深度学习》第二章拾遗 | unsafe fn</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="zh_cn" href="../../rss.xml">
<link rel="canonical" href="https://mklf.github.io/posts/shen-du-xue-xi-di-er-zhang-shi-yi/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="mklf">
<link rel="prev" href="../chou-yang-fang-fa-2-cong-gao-si-fen-bu-shuo-qi/" title="抽样方法(2)--拒绝抽样和高斯分布" type="text/html">
<meta property="og:site_name" content="unsafe fn">
<meta property="og:title" content="《深度学习》第二章拾遗">
<meta property="og:url" content="https://mklf.github.io/posts/shen-du-xue-xi-di-er-zhang-shi-yi/">
<meta property="og:description" content="最近在阅读《深度学习》这本书。可能限于篇幅，有很多结论作者没有详细说明。所以在博客记录一下我对这些结论的推导。








原文2.7节 实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^TAx$，其中 限制 $\|x \|^2 = 1$。当 x 等于 A 的某个特征向量时，f 将返回对应的特征值。在限制条 件下，函数 f 的最大值是最大特征值，最小值是最小特征值。
证明
首先把">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-01-16T18:28:27+08:00">
<meta property="article:tag" content="math">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="线性代数">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">跳到主内容</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">unsafe fn</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav"></ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">文章归档</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">标签</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS 源</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">《深度学习》第二章拾遗</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    mklf
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2018-01-16T18:28:27+08:00" itemprop="datePublished" title="2018-01-16 18:28">2018-01-16 18:28</time></a>
            </p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/shen-du-xue-xi-di-er-zhang-shi-yi.html">评论</a>


            

        </p>
</div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>最近在阅读《深度学习》这本书。可能限于篇幅，有很多结论作者没有详细说明。所以在博客记录一下我对这些结论的推导。
<!-- TEASER_END --></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>原文2.7节</strong> 实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^TAx$，其中 限制 $\|x \|^2 = 1$。当 x 等于 A 的某个特征向量时，f 将返回对应的特征值。在限制条 件下，函数 f 的最大值是最大特征值，最小值是最小特征值。</p>
<p><strong>证明</strong></p>
<p>首先把问题表示成标准形式:
$$\min(\max)\quad f(x)=x^TAx$$
$$s.t.\quad\|x\|^2-1=0$$</p>
<p>用拉格朗日乘子法:</p>
<p>注意到$\|x\|^2=1$ 等价于 $x^Tx=1$ 拉格朗日函数为:
$$l(x,\lambda) = f(x)-\lambda (x^Tx-1)$$</p>
<p>上式对$x$求导，注意$A$是一个对称矩阵,得到:</p>
\begin{equation}
\begin{split} 
\frac{\delta l}{\delta x}&amp;=Ax+A^Tx-2\lambda x\\\\
\quad&amp;=2(A-\lambda\cdot I)x\\\\
\end{split}
\end{equation}<p>令导数为0，得到:
$$(A-\lambda\cdot I)x=0$$</p>
<p>观察这个等式的结构可以发现这就是矩阵的特征值和特征向量的求解公式，因此$\lambda$就等于特征值，$x$是特征向量单位化的值。回顾特征值和特征向量满足$Ax=\lambda x$，注意到$x^Tx=1$,因此:
$$f(x)=x^TAx=x\cdot\lambda x=\lambda x^Tx = \lambda$$</p>
<p>因此，和2.7节的结论一致，$f(x)$的最大值是最大的特征值，最小值是最小的特征值，此时$x$是对应特征值的特征向量。</p>
<hr>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>原文2.8节</strong> 我们可以用与 A 相关的特征分解去解释$A$的奇异值分解。$A$的左奇异向量（left singular vector）是 $AA^T$ 的特征向量。$A$的右奇异向量（right singular vector）是$A^TA$的特征向量。$A$的非零奇异值是$A^TA$特征值的平方根，同时也是$AA^T$特征值的平方根。</p>
<p><strong>证明</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我们知道如果A为方阵，它的特征值$\lambda$和特征向量$x$满足
$$Ax = \lambda x$$
如果A是一个$m\times n$维的矩阵，$v$ 是一个n维向量，$u$是一个m维向量,它的奇异值分解满足:
$$Av=\lambda u$$
$$A^Tu = \lambda v$$
奇异值分解的求解方法是:</p>
$$Av=\lambda u$$<p>则:
$$A^TAv=\lambda A^Tu = \lambda ^2 v$$
上式表明，$v$ 是$A^TA$的特征向量，同理$u$ 是$AA^T$的特征向量,和上文的描述一致。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>这个结论的一个用处是帮助理解PCA和SVD之间的关系</p>
<p>给定一个中心化(即均值为0)的数据矩阵$X_{n\times m}$，其中$n$是数据的数目，$m$是数据的维度，PCA 首先求数据的协方差矩阵，然后对协方差矩阵进行特征分解，然后按特征值由大到小进行选择。形式化来说，由于$X$已经进行过中心化，因此$X$的协方差矩阵可以表示为$\frac{1}{n-1}X^TX$,然后PCA对矩阵$\frac{1}{n}X^TX$进行特征分解。</p>
<p>上文已经证明过$X^TX$的特征值就是$X$的右奇异向量,因此PCA得到的特征向量就是$X$的右奇异向量。而对于特征向量，注意到协方差矩阵前面有一个$\frac{1}{n-1}$,因此PCA特征值的$n-1$倍等于SVD特征值的平方。</p>
<p>我们可以用下面的代码来验证上述关系。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 得到一个4*4 的矩阵</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># 中心化</span>
<span class="n">X</span> <span class="o">-=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="c1"># PCA</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">41</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 特征值和特征向量</span>
    <span class="n">eigen_val</span><span class="p">,</span><span class="n">eigen_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="c1"># 按从大到小排序</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">eigen_val</span><span class="p">[</span><span class="n">order</span><span class="p">],</span><span class="n">eigen_vec</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># SVD 返回的是U\sigmaV^T,我们不需要U</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">pca_val</span><span class="p">,</span><span class="n">pca_vec</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">svd_val</span><span class="p">,</span><span class="n">svd_vec</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"PCA 特征值的n-1倍是:"</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">pca_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"SVD 特征值的二次方是:"</span><span class="p">,</span><span class="n">svd_val</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>PCA 特征值的n-1倍是: [  8.30037519e+00   1.92882619e+00   3.63183940e-02  -2.22306907e-16]
SVD 特征值的二次方是: [  8.30037519e+00   1.92882619e+00   3.63183940e-02   1.88715574e-33]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>原文2.9节</strong> Moore-Penrose 伪逆的补充</p>
<p>给定一个矩阵$A_{n\times m}$，A的 Moore-Penrose 伪逆定义为$A^*=(A^TA)^{-1}A^T$</p>
<p>伪逆有以下特性:</p>
<ol>
<li>$A^*A = (A^TA)^{-1}A^TA=I$，这也是$A*$被称作伪逆的原因</li>
<li>$A^*A$ 是一个投影矩阵,OLS $Ax=b$的一种解释是找到距离A的值域中距离b最近的投影，这里采用的投影矩阵就是$A^*A$</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>原文2.12节</strong> $argmax_d{Tr(d^TX^TXd)}$ subject to $d^Td=1$ 这个优化问题可以通过特征分解来求解。最优的$d$就是$X^TX$最大的特征值对应的特征向量</p>
<p><strong>证明</strong></p>
<p>注意到$d^TX^TXd$ 是一个标量,因此$Tr(d^TX^TXd)$ 的迹运算可以去掉，设$A=X^TX$，注意到A是一个对称矩阵，那么原问题变为:
$$\mathop{\arg\max}_d\ d^TAd$$
$$s.t.\quad d^Td=1$$
这个问题就变为上文2.7节中讨论的问题，目标函数的最大值就是矩阵$A=X^TX$的最大特征值，而$d$是最大特征值对应的特征向量的单位化。</p>

</div>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/math/" rel="tag">math</a></li>
            <li><a class="tag p-category" href="../../categories/shen-du-xue-xi/" rel="tag">深度学习</a></li>
            <li><a class="tag p-category" href="../../categories/xian-xing-dai-shu/" rel="tag">线性代数</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../chou-yang-fang-fa-2-cong-gao-si-fen-bu-shuo-qi/" rel="prev" title="抽样方法(2)--拒绝抽样和高斯分布">上一篇文章</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>评论</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="mklf-github-io",
            disqus_url="https://mklf.github.io/posts/shen-du-xue-xi-di-er-zhang-shi-yi/",
        disqus_title="\u300a\u6df1\u5ea6\u5b66\u4e60\u300b\u7b2c\u4e8c\u7ae0\u62fe\u9057",
        disqus_identifier="cache/posts/shen-du-xue-xi-di-er-zhang-shi-yi.html",
        disqus_config = function () {
            this.language = "zh_cn";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
     },
     displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
     "HTML-CSS": {
         styles: {'.MathJax_Display': {"margin": 0}}
     }
 });
 </script></article><script>var disqus_shortname="mklf-github-io";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer">
            Contents © 2021         <a href="mailto:fangli9@outlook.com">mklf</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
