<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="zh_cn">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]--><!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]--><!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]--><!--[if gt IE 8]><!--><!--<![endif]--><head>
<meta charset="utf-8">
<!-- http://t.co/dKP3o1e --><meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>《深度学习》第二章拾遗 | MKLF</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/main.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/extra.css" rel="stylesheet" type="text/css">
<!-- Webfonts --><link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!-- Load Modernizr --><script src="../../assets/js/modernizr-2.6.2.custom.min.js"></script><link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://blog.fangli.org/posts/shen-du-xue-xi-di-er-zhang-shi-yi/">
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
     },
     Tex:{
        equationNumbers: { autoNumber: "AMS" }
     },
     showMathMenu: false,
     showMathMenuMISE:false,
     messageStyle:"none",
     displayAlign: 'center', // Change this to 'center' to center equations.
     "HTML-CSS": {
         styles: {'.MathJax_Display': {"margin": 0}},
         linebreaks:{automatic:true,width:"container"}
     },
     SVG:{linebreaks:{automatic:true}}
 });
 </script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css">
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="author" content="Frank Lee">
<link rel="prev" href="../chou-yang-fang-fa-2-cong-gao-si-fen-bu-shuo-qi/" title="抽样方法(2)--拒绝抽样和高斯分布" type="text/html">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><meta property="og:site_name" content="MKLF">
<meta property="og:title" content="《深度学习》第二章拾遗">
<meta property="og:url" content="https://blog.fangli.org/posts/shen-du-xue-xi-di-er-zhang-shi-yi/">
<meta property="og:description" content="最近在阅读《深度学习》这本书。可能限于篇幅，有很多结论作者没有详细说明。所以在博客记录一下我对这些结论的推导。









原文2.7节 实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^TAx$，其中 限制 $\|x \|^2 = 1$。当 x 等于 A 的某个特征向量时，f 将返回对应的特征值。在限制条 件下，函数 f 的最大值是最大特征值，最小值是最小特征值。
证明
首先">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-01-16T18:28:27+08:00">
<meta property="article:tag" content="math">
</head>
<body id="post-index" class="feature">
<a href="#content" class="sr-only sr-only-focusable">跳到主内容</a>
    

    <nav id="dl-menu" class="dl-menuwrapper" role="navigation"><button class="dl-trigger">Open Menu</button>
    <ul class="dl-menu">
<li><a href="../../">首页</a></li>
            <li><a href="../../archive.html">存档</a></li>
            <li><a href="../../categories/">分类</a></li>
            <li><a href="../../rss.xml">RSS</a></li>
    
    
    </ul></nav><div class="entry-header">
    <div class="entry-image">
      <img src="../../images/beijing.jpg" alt="Latest Posts">
</div>
<!-- /.entry-image -->
      
  <div class="header-title">
    <div class="header-title-wrap">
    <h1 id="brand"><a href="https://blog.fangli.org/" title="MKLF" rel="home">
    MKLF</a></h1>
     <!-- <h2>《深度学习》第二章拾遗</h2>  -->
    </div>
<!-- /.header-title-wrap -->
  </div>
<!-- /.header-title -->

</div>
<!-- /.entry-header -->

    <div id="main" role="main">
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><div class="entry-meta">
            <span class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2018-01-16T18:28:27+08:00" itemprop="datePublished" title="2018-01-16 18:28">2018-01-16 18:28</time></a></span>  ·  
            <span class="byline author vcard">Frank Lee</span>  ·  
            

        </div>
        

    
    <h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">《深度学习》第二章拾遗</a></h1>

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div id="notebook" class="border-box-sizing">
    <div id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>最近在阅读《深度学习》这本书。可能限于篇幅，有很多结论作者没有详细说明。所以在博客记录一下我对这些结论的推导。
<!-- TEASER_END --></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>原文2.7节</strong> 实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^TAx$，其中 限制 $\|x \|^2 = 1$。当 x 等于 A 的某个特征向量时，f 将返回对应的特征值。在限制条 件下，函数 f 的最大值是最大特征值，最小值是最小特征值。</p>
<p><strong>证明</strong></p>
<p>首先把问题表示成标准形式:
$$\min(\max)\quad f(x)=x^TAx$$
$$s.t.\quad\|x\|^2-1=0$$</p>
<p>用拉格朗日乘子法:</p>
<p>注意到$\|x\|^2=1$ 等价于 $x^Tx=1$ 拉格朗日函数为:
$$l(x,\lambda) = f(x)-\lambda (x^Tx-1)$$</p>
<p>上式对$x$求导，注意$A$是一个对称矩阵,得到:</p>
<p>\begin{equation}
\begin{split} 
\frac{\delta l}{\delta x}&amp;=Ax+A^Tx-2\lambda x\\\\
\quad&amp;=2(A-\lambda\cdot I)x\\\\
\end{split}
\end{equation}</p>
<p>令导数为0，得到:
$$(A-\lambda\cdot I)x=0$$</p>
<p>观察这个等式的结构可以发现这就是矩阵的特征值和特征向量的求解公式，因此$\lambda$就等于特征值，$x$是特征向量单位化的值。回顾特征值和特征向量满足$Ax=\lambda x$，注意到$x^Tx=1$,因此:
$$f(x)=x^TAx=x\cdot\lambda x=\lambda x^Tx = \lambda$$</p>
<p>因此，和2.7节的结论一致，$f(x)$的最大值是最大的特征值，最小值是最小的特征值，此时$x$是对应特征值的特征向量。</p>
<hr>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>原文2.8节</strong> 我们可以用与 A 相关的特征分解去解释$A$的奇异值分解。$A$的左奇异向量（left singular vector）是 $AA^T$ 的特征向量。$A$的右奇异向量（right singular vector）是$A^TA$的特征向量。$A$的非零奇异值是$A^TA$特征值的平方根，同时也是$AA^T$特征值的平方根。</p>
<p><strong>证明</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我们知道如果A为方阵，它的特征值$\lambda$和特征向量$x$满足
$$Ax = \lambda x$$
如果A是一个$m\times n$维的矩阵，$v$ 是一个n维向量，$u$是一个m维向量,它的奇异值分解满足:
$$Av=\lambda u$$
$$A^Tu = \lambda v$$
奇异值分解的求解方法是:</p>
<p>$$Av=\lambda u$$
则:
$$A^TAv=\lambda A^Tu = \lambda ^2 v$$
上式表明，$v$ 是$A^TA$的特征向量，同理$u$ 是$AA^T$的特征向量,和上文的描述一致。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>这个结论的一个用处是帮助理解PCA和SVD之间的关系</p>
<p>给定一个中心化(即均值为0)的数据矩阵$X_{n\times m}$，其中$n$是数据的数目，$m$是数据的维度，PCA 首先求数据的协方差矩阵，然后对协方差矩阵进行特征分解，然后按特征值由大到小进行选择。形式化来说，由于$X$已经进行过中心化，因此$X$的协方差矩阵可以表示为$\frac{1}{n-1}X^TX$,然后PCA对矩阵$\frac{1}{n}X^TX$进行特征分解。</p>
<p>上文已经证明过$X^TX$的特征值就是$X$的右奇异向量,因此PCA得到的特征向量就是$X$的右奇异向量。而对于特征向量，注意到协方差矩阵前面有一个$\frac{1}{n-1}$,因此PCA特征值的$n-1$倍等于SVD特征值的平方。</p>
<p>我们可以用下面的代码来验证上述关系。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># 得到一个4*4 的矩阵</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># 中心化</span>
<span class="n">X</span> <span class="o">-=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="c1"># PCA</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">41</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 特征值和特征向量</span>
    <span class="n">eigen_val</span><span class="p">,</span><span class="n">eigen_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="c1"># 按从大到小排序</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigen_val</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">eigen_val</span><span class="p">[</span><span class="n">order</span><span class="p">],</span><span class="n">eigen_vec</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># SVD 返回的是U\sigmaV^T,我们不需要U</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">pca_val</span><span class="p">,</span><span class="n">pca_vec</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">svd_val</span><span class="p">,</span><span class="n">svd_vec</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"PCA 特征值的n-1倍是:"</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">pca_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"SVD 特征值的二次方是:"</span><span class="p">,</span><span class="n">svd_val</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>PCA 特征值的n-1倍是: [  8.30037519e+00   1.92882619e+00   3.63183940e-02  -2.22306907e-16]
SVD 特征值的二次方是: [  8.30037519e+00   1.92882619e+00   3.63183940e-02   1.88715574e-33]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>原文2.9节</strong> Moore-Penrose 伪逆的补充</p>
<p>给定一个矩阵$A_{n\times m}$，A的 Moore-Penrose 伪逆定义为$A^*=(A^TA)^{-1}A^T$</p>
<p>伪逆有以下特性:</p>
<ol>
<li>$A^*A = (A^TA)^{-1}A^TA=I$，这也是$A*$被称作伪逆的原因</li>
<li>$A^*A$ 是一个投影矩阵,OLS $Ax=b$的一种解释是找到距离A的值域中距离b最近的投影，这里采用的投影矩阵就是$A^*A$</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>原文2.12节</strong> $argmax_d{Tr(d^TX^TXd)}$ subject to $d^Td=1$ 这个优化问题可以通过特征分解来求解。最优的$d$就是$X^TX$最大的特征值对应的特征向量</p>
<p><strong>证明</strong></p>
<p>注意到$d^TX^TXd$ 是一个标量,因此$Tr(d^TX^TXd)$ 的迹运算可以去掉，设$A=X^TX$，注意到A是一个对称矩阵，那么原问题变为:
$$argmax_d\ d^TAd$$
$$s.t.\quad d^Td=1$$
这个问题就变为上文2.7节中讨论的问题，它的最大值就是矩阵$A=X^TX$的特征向量。</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li>
            <a class="fa fa-tags" href="../../categories/math/" rel="tag">math</a>
        </li>
        </ul>
<ul class="pager hidden-print">
<li class="fa fa-book">
                <a href="../chou-yang-fang-fa-2-cong-gao-si-fen-bu-shuo-qi/" rel="prev" title="抽样方法(2)--拒绝抽样和高斯分布">前一篇: 抽样方法(2)--拒绝抽样和高斯分布</a>
            </li>
        </ul></nav></aside><section class="comments"><script>
var idcomments_acct = 'c1e905419bafc09e7b5cf91a7a3b1e66';
var idcomments_post_id = "cache/posts/shen-du-xue-xi-di-er-zhang-shi-yi.html";
var idcomments_post_url = "https://blog.fangli.org/posts/shen-du-xue-xi-di-er-zhang-shi-yi/";
</script><span id="IDCommentsPostTitle" style="display:none"></span>
<script src="https://www.intensedebate.com/js/genericCommentWrapperV2.js"></script></section><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
     },
     Tex:{
        equationNumbers: { autoNumber: "AMS" }
     },
     showMathMenu: false,
     showMathMenuMISE:false,
     messageStyle:"none",
     displayAlign: 'center', // Change this to 'center' to center equations.
     "HTML-CSS": {
         styles: {'.MathJax_Display': {"margin": 0}},
         linebreaks:{automatic:true,width:"container"}
     },
     SVG:{linebreaks:{automatic:true}}
 });
 </script></article>
</div>
    
    <div class="footer-wrapper">
        <footer role="contentinfo"><p>Contents © 2018         <a href="mailto:golifang123@163.com">Frank Lee</a>- Powered by Nikola theme hpstr         <span id="busuanzi_container_site_pv">pv:<span id="busuanzi_value_site_pv"></span></span></p>
            
        </footer>
</div>

    
<script>$("div.output_png > img").colorbox({html:function(){ return "<div><img src="+this.src+"></div>"} });
</script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script><script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script><script src="../../assets/js/scripts.min.js"></script>
</body>
</html>
